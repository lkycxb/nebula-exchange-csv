# Use the command to submit the exchange job:

# spark-submit \
# --master "spark://master_ip:7077" \
# --driver-memory=2G --executor-memory=30G  \
# --num-executors=3 --executor-cores=20 \
# --class com.vesoft.nebula.exchange.Exchange \
# nebula-exchange-3.0-SNAPSHOT.jar -c csv_datasource.conf

{
  # Spark config
  spark: {
    app: {
      name: NebulaGraph Exchange
      master: "loal[2]"
    }
  }

  # Nebula Graph config
  nebula: {
    address:{
      graph:["192.88.1.241:9669"]
      # if your NebulaGraph server is in virtual network like k8s, please config the leader address of meta.
      # use `SHOW meta leader` to see your meta leader's address
      meta:["192.88.1.241:9559"]
    }
    user: root
    pswd: nebula
    space: compographtes1

    path:{
        # any path that owns read and write access is ok
        local:"F:/share/vulncomponents/nebula/compograph/local"
        remote:"F:/share/vulncomponents/nebula/compograph/csv"
        hdfs.namenode: "file:///"
        separator: ","
    }

    # nebula client connection parameters
    connection {
      # socket connect & execute timeout, unit: millisecond
      timeout: 30000
    }

    error: {
      # max number of failures, if the number of failures is bigger than max, then exit the application.
      max: 32
      # failed data will be recorded in output path, format with ngql
      output: /tmp/errors
    }

    # use google's RateLimiter to limit the requests send to NebulaGraph
    rate: {
      # the stable throughput of RateLimiter
      limit: 1024
      # Acquires a permit from RateLimiter, unit: MILLISECONDS
      # if it can't be obtained within the specified timeout, then give up the request.
      timeout: 1000
    }
  }

  # Processing tags
  tags: [
    {
      name: testtag
      type: {
        source: nebula
        sink: csv
      }
      #exec: ""
      noField:false
      fields: [dlong,dbool,dstr1,dstr2,ddouble,dint,dshort,dbyte,dfloat,ddate,dtime,ddatetime,dtimestamp,dgeo,dgeopoint,dgeoline,dgeopoly]
      nebula.fields: [dlong,dbool,dstr1,dstr2,ddouble,dint,dshort,dbyte,dfloat,ddate,dtime,ddatetime,dtimestamp,dgeo,dgeopoint,dgeoline,dgeopoly]
      vertex: _vertexId
      batch: 2000
      partition: 60

    }
  ]

  # process edges
  edges: [
    {
      name: testedge
      type: {
        source: nebula
        sink: csv
      }
      #exec: "select hive-field0, hive-field1, hive-field2 from database.table"
      fields: [dlong,dbool,dstr1,dstr2,ddouble,dint,dshort,dbyte,dfloat,ddate,dtime,ddatetime,dtimestamp,dgeo,dgeopoint,dgeoline,dgeopoly]
      nebula.fields: [dlong,dbool,dstr1,dstr2,ddouble,dint,dshort,dbyte,dfloat,ddate,dtime,ddatetime,dtimestamp,dgeo,dgeopoint,dgeoline,dgeopoly]
      #_srcId, _dstId, _rank
      source: _srcId
      target: _dstId
      #ranking: dt
      batch: 2000
      partition: 60

    }
  ]
}
